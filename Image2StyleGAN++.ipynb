{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os,sys,cv2\n",
    "import copy\n",
    "import keras\n",
    "sys.path.append(os.pardir)\n",
    "from KerasGANs.Keras_StyleGAN import *\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend import tensorflow_backend as K\n",
    "\n",
    "cf = tf.ConfigProto(\n",
    "    gpu_options=tf.GPUOptions(\n",
    "        allow_growth=True\n",
    "    )\n",
    ")\n",
    "session = tf.Session(config=cf)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "基本いじらんでよい  \n",
    "Target / GANのresが224以下の時はそれ以下に編集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_INPUT = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting\n",
    "target_image_path : Embする画像のパス  \n",
    "weights_path      : Embする空間をもつGANのweight(Keras)パス  \n",
    "save_image_path   : 途中生成の画像を保存するパス  \n",
    "save_image_name   : 途中生成の画像の名前  \n",
    "res               : GANの出力サイズ  \n",
    "stage             : GANのStageの数(1024だったら18 / 512だったら16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image_path   = \"IMG_2791.jpg\"\n",
    "weights_path        = \"-----.h5\"\n",
    "save_image_path     = \"IMG_2791/\"\n",
    "save_image_name     = \"IMG_2791\"\n",
    "save_latent_path    = \"IMG_2791_dlatent.npy\"\n",
    "res = 1024\n",
    "stage = 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "##        Prepare dlatent be trainable      ##\n",
    "##############################################\n",
    "def dlatent_model(init):\n",
    "    def myinit(shape, dtype=None):\n",
    "        return init[np.newaxis]\n",
    "        \n",
    "    inp = Input((1,))\n",
    "    output = [LeakyReLU(0.2)(Dense(512,use_bias=False,name = \"w_%i\"%(i),kernel_initializer=myinit)(inp)) for i in range(stage)]\n",
    "    output = Lambda(lambda x: tf.reshape(x,(-1,stage,512)))(output)\n",
    "    return Model(inp,output)\n",
    "\n",
    "##############################################\n",
    "##        Prepare VGG model                  ##\n",
    "##############################################\n",
    "def VGG_model():\n",
    "    input_tensor = Input(shape=(VGG_INPUT, VGG_INPUT, 3))\n",
    "    VGG = VGG16(include_top=False, weights='imagenet', input_tensor=input_tensor)\n",
    "    out1 = Flatten()(VGG.get_layer(\"block1_conv1\").output)\n",
    "    out2 = Flatten()(VGG.get_layer(\"block1_conv2\").output)\n",
    "    out3 = Flatten()(VGG.get_layer(\"block2_conv2\").output)\n",
    "    out4 = Flatten()(VGG.get_layer(\"block3_conv3\").output)\n",
    "    out  = Concatenate()([out1,out2,out3,out4]) \n",
    "\n",
    "    return Model(input_tensor,out)\n",
    "\n",
    "##############################################\n",
    "## Prepare Image Generator(Style Based Gan) ##\n",
    "##############################################\n",
    "GAN = StyleGAN(output_res = res)\n",
    "GAN.s_model.load_weights(weights_path)\n",
    "\n",
    "# Instance\n",
    "w_model = dlatent_model(np.random.normal(size=(512)))\n",
    "g_model = GAN.synthesis_model()\n",
    "v_model = VGG_model()\n",
    "\n",
    "\n",
    "# Define Model\n",
    "noise_inp     = [Input((g_model.input[1:][i].shape[1:])) for i in range(len(g_model.input)-1)]\n",
    "inp           = Input((1,))\n",
    "noise_inp     = [Input((g_model.input[1:][i].shape[1:])) for i in range(len(g_model.input)-1)]\n",
    "embedding_w   = w_model(inp)\n",
    "Generated_img = g_model([embedding_w] + noise_inp)\n",
    "Reshape_img   = Lambda(lambda x: tf.image.resize(x,(res,res)))(Generated_img)\n",
    "vgg_feature   = v_model(Reshape_img)\n",
    "model = Model([inp]+noise_inp , [vgg_feature,Generated_img])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define In/Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Target\n",
    "img = cv2.imread(target_image_path)[:,:,::-1]\n",
    "t_img = cv2.resize(img,(res,res))\n",
    "img = cv2.resize(img,(VGG_INPUT,VGG_INPUT))                                                          #resize_img\n",
    "target_feature = v_model.predict(preprocess_input(copy.deepcopy(img)[np.newaxis]))           #F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Input\n",
    "one  = np.ones((1,))\n",
    "noise = [np.random.normal(size=(1,w,w,1)) for w in [2**(int(i/2)+2) for i in range(stage)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true,y_label):\n",
    "    return tf.sqrt(tf.reduce_mean((y_true - y_label)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=0.00000001, decay=0.0, amsgrad=False)\n",
    "model.compile(adam,[rmse,rmse],loss_weights=[0.00001,0.00001])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result epochs100 : [0.004115925636142492, 331.44970703125, 80.14288330078125]\n",
      "Result epochs200 : [0.0038880875799804926, 326.60699462890625, 62.2017822265625]\n",
      "Result epochs300 : [0.0037314353976398706, 324.468994140625, 48.67454528808594]\n",
      "Result epochs400 : [0.0037270979955792427, 321.3507995605469, 51.3590202331543]\n",
      "Result epochs500 : [0.0035953596234321594, 317.2538757324219, 42.282108306884766]\n",
      "Result epochs600 : [0.0035848510451614857, 312.4703369140625, 46.0147705078125]\n",
      "Result epochs700 : [0.003500476013869047, 309.8905029296875, 40.1571044921875]\n",
      "Result epochs800 : [0.003449872136116028, 307.5269775390625, 37.460227966308594]\n",
      "Result epochs900 : [0.003370601451024413, 303.17022705078125, 33.88991165161133]\n",
      "Result epochs1000 : [0.00330721284262836, 301.14239501953125, 29.578876495361328]\n",
      "Result epochs1100 : [0.003296144073829055, 299.3504638671875, 30.26395034790039]\n",
      "Result epochs1200 : [0.0032838478218764067, 299.3710632324219, 29.01374053955078]\n",
      "Result epochs1300 : [0.003250125329941511, 295.5690612792969, 29.4434814453125]\n",
      "Result epochs1400 : [0.003191363997757435, 292.7669982910156, 26.369400024414062]\n",
      "Result epochs1500 : [0.003162871114909649, 290.1778869628906, 26.109216690063477]\n",
      "Result epochs1600 : [0.0031456334982067347, 290.2762145996094, 24.287137985229492]\n",
      "Result epochs1700 : [0.003125457325950265, 288.8675842285156, 23.678157806396484]\n",
      "Result epochs1800 : [0.0030403148848563433, 283.71124267578125, 20.32023811340332]\n",
      "Result epochs1900 : [0.0030574665870517492, 285.4442138671875, 20.30245590209961]\n",
      "Result epochs2000 : [0.0030243657529354095, 283.188232421875, 19.248342514038086]\n",
      "Result epochs2100 : [0.003110840916633606, 290.2692565917969, 20.814847946166992]\n",
      "Result epochs2200 : [0.0029554872307926416, 278.8498840332031, 16.69884490966797]\n",
      "Result epochs2300 : [0.002924823435023427, 276.8528137207031, 15.629549026489258]\n",
      "Result epochs2400 : [0.0029768762178719044, 280.5834045410156, 17.104202270507812]\n",
      "Result epochs2500 : [0.002887769602239132, 274.5120544433594, 14.264912605285645]\n",
      "Result epochs2600 : [0.0028717268723994493, 274.29217529296875, 12.880510330200195]\n",
      "Result epochs2700 : [0.002872731303796172, 273.9551086425781, 13.318016052246094]\n",
      "Result epochs2800 : [0.0028853858821094036, 274.6481628417969, 13.890450477600098]\n",
      "Result epochs2900 : [0.0028406211640685797, 272.0103759765625, 12.05174446105957]\n",
      "Result epochs3000 : [0.0028477050364017487, 272.8835754394531, 11.886919021606445]\n",
      "Result epochs3100 : [0.0028274809010326862, 271.5582275390625, 11.18986701965332]\n",
      "Result epochs3200 : [0.002820324618369341, 271.3869323730469, 10.645525932312012]\n",
      "Result epochs3300 : [0.0028186943382024765, 271.240966796875, 10.628490447998047]\n",
      "Result epochs3400 : [0.0028233930934220552, 271.6514587402344, 10.68786334991455]\n",
      "Result epochs3500 : [0.002849238459020853, 272.63677978515625, 12.287090301513672]\n",
      "Result epochs3600 : [0.0027995037380605936, 270.3151550292969, 9.635235786437988]\n",
      "Result epochs3700 : [0.002847292460501194, 272.7038269042969, 12.025413513183594]\n",
      "Result epochs3800 : [0.0027908128686249256, 269.87139892578125, 9.209914207458496]\n",
      "Result epochs3900 : [0.002793953288346529, 270.26300048828125, 9.132308959960938]\n",
      "Result epochs4000 : [0.0027843713760375977, 269.8383483886719, 8.598775863647461]\n",
      "Result epochs4100 : [0.002785678720101714, 269.9905090332031, 8.577369689941406]\n",
      "Result epochs4200 : [0.0027736679185181856, 269.3581237792969, 8.008662223815918]\n",
      "Result epochs4300 : [0.002777363872155547, 269.5030517578125, 8.233329772949219]\n",
      "Result epochs4400 : [0.0027780497912317514, 269.6842041015625, 8.120800971984863]\n",
      "Result epochs4500 : [0.0027746919076889753, 269.6195068359375, 7.849690914154053]\n",
      "Result epochs4600 : [0.002781025832518935, 269.9693908691406, 8.133194923400879]\n",
      "Result epochs4700 : [0.002779555507004261, 269.68133544921875, 8.2742280960083]\n",
      "Result epochs4800 : [0.002766914665699005, 269.22998046875, 7.461483001708984]\n",
      "Result epochs4900 : [0.002766073914244771, 269.29302978515625, 7.314359188079834]\n",
      "Result epochs5000 : [0.002756675938144326, 268.5868835449219, 7.080713748931885]\n"
     ]
    }
   ],
   "source": [
    "iteration = 5000\n",
    "for i in range(iteration):\n",
    "    model.train_on_batch(x=[one]+noise,y=[target_feature,t_img[np.newaxis]])\n",
    "    if i%100 == 99:\n",
    "        print(\"Result epochs%i : \"%(i+1),end=\"\")\n",
    "        print(model.evaluate(x=[one]+noise,y=[target_feature,t_img[np.newaxis]],verbose=0))\n",
    "        img = model.predict([one]+noise)[1][0][:,:,::-1]\n",
    "        img = np.array(np.clip(img,0,255),dtype=\"uint8\")\n",
    "        cv2.imwrite(save_image_path + save_image_name +\"_%05i.jpg\"%(i+1),img)\n",
    "pred = w_model.predict(one)\n",
    "np.save(save_latent_path,pred[0,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_imai)",
   "language": "python",
   "name": "conda_imai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
